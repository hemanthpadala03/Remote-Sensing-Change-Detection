{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Lr-3O0T3iOQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Dense, Flatten, Multiply, Add, BatchNormalization, Dropout\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Helper function to load and pad images\n",
        "def load_and_pad_image(filepath, pad_width):\n",
        "    image = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "    return np.pad(image, ((pad_width, pad_width), (pad_width, pad_width)), mode='constant')\n",
        "\n",
        "# Function to get neighborhood\n",
        "def get_neighbourhood(image_padded, i, j, pad_width):\n",
        "    return image_padded[i - pad_width:i + pad_width + 1, j - pad_width:j + pad_width + 1].reshape(5, 5, 1)\n",
        "\n",
        "# Function to prepare data for a given dataset\n",
        "def prepare_data(image1, image2, ground_truth, pad_width):\n",
        "    image1_padded = load_and_pad_image(image1, pad_width)\n",
        "    image2_padded = load_and_pad_image(image2, pad_width)\n",
        "    ground_truth = cv2.imread(ground_truth, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    for i in range(pad_width, image1_padded.shape[0] - pad_width):\n",
        "        for j in range(pad_width, image1_padded.shape[1] - pad_width):\n",
        "            neighbourhood1 = get_neighbourhood(image1_padded, i, j, pad_width)\n",
        "            neighbourhood2 = get_neighbourhood(image2_padded, i, j, pad_width)\n",
        "            data.append(np.dstack((neighbourhood1, neighbourhood2)))\n",
        "            labels.append(1 if ground_truth[i - pad_width, j - pad_width] == 0 else 0)\n",
        "\n",
        "    return np.array(data), np.array(labels), ground_truth\n",
        "\n",
        "# Define attention gate\n",
        "def attention_gate(x, g, inter_channels):\n",
        "    theta_x = Conv2D(inter_channels, kernel_size=1, strides=1, padding='same')(x)\n",
        "    phi_g = Conv2D(inter_channels, kernel_size=1, strides=1, padding='same')(g)\n",
        "    add_xg = Add()([theta_x, phi_g])\n",
        "    relu_xg = tf.keras.layers.Activation('relu')(add_xg)\n",
        "    psi = Conv2D(1, kernel_size=1, strides=1, padding='same', activation='sigmoid')(relu_xg)\n",
        "    return Multiply()([x, psi])\n",
        "\n",
        "# Define U-Net with attention model\n",
        "def unet_with_attention(input_shape=(5, 5, 2)):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Dropout(0.2)(conv1)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D((1, 1))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Dropout(0.2)(conv2)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D((1, 1))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Dropout(0.2)(conv3)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Dropout(0.2)(conv4)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "\n",
        "    aspp_1 = Conv2D(256, (3, 3), dilation_rate=1, activation='relu', padding='same')(conv4)\n",
        "    aspp_2 = Conv2D(256, (3, 3), dilation_rate=2, activation='relu', padding='same')(conv4)\n",
        "    aspp_3 = Conv2D(256, (3, 3), dilation_rate=4, activation='relu', padding='same')(conv4)\n",
        "    aspp_4 = Conv2D(256, (3, 3), dilation_rate=8, activation='relu', padding='same')(conv4)\n",
        "    aspp_out = Concatenate()([aspp_1, aspp_2, aspp_3, aspp_4])\n",
        "    aspp_out = Conv2D(256, (1, 1), activation='relu', padding='same')(aspp_out)\n",
        "\n",
        "    up5 = UpSampling2D((1, 1))(aspp_out)\n",
        "    att5 = attention_gate(conv3, up5, inter_channels=128)\n",
        "    concat5 = Concatenate()([up5, att5])\n",
        "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(concat5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "\n",
        "    up6 = UpSampling2D((1, 1))(conv5)\n",
        "    att6 = attention_gate(conv2, up6, inter_channels=64)\n",
        "    concat6 = Concatenate()([up6, att6])\n",
        "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(concat6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "\n",
        "    up7 = UpSampling2D((1, 1))(conv6)\n",
        "    att7 = attention_gate(conv1, up7, inter_channels=32)\n",
        "    concat7 = Concatenate()([up7, att7])\n",
        "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(concat7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "\n",
        "    output = Flatten()(conv7)\n",
        "    output = Dense(1, activation='sigmoid')(output)\n",
        "    model = Model(inputs, output)\n",
        "    return model\n",
        "\n",
        "# Define datasets\n",
        "datasets = [\n",
        "    (\"/content/set11.tif\", \"/content/set12.tif\", \"/content/gt1.tif\"),\n",
        "    (\"/content/set21.tif\", \"/content/set22.tif\", \"/content/gt2.tif\"),\n",
        "    (\"/content/set31.tif\", \"/content/set32.tif\", \"/content/gt3.tif\"),\n",
        "    (\"/content/set41.tif\", \"/content/set42.tif\", \"/content/gt4.tif\"),\n",
        "    (\"/content/set51.tif\", \"/content/set52.tif\", \"/content/gt5.tif\"),\n",
        "    (\"/content/set61.tif\", \"/content/set62.tif\", \"/content/gt6.tif\")\n",
        "]\n",
        "\n",
        "# Split datasets into training and testing sets\n",
        "train_datasets = datasets[:3]\n",
        "test_datasets = datasets[3:]\n",
        "\n",
        "# Prepare training data\n",
        "pad_width = 2\n",
        "X_train, y_train = [], []\n",
        "\n",
        "for img1_path, img2_path, gt_path in train_datasets:\n",
        "    data, labels, _ = prepare_data(img1_path, img2_path, gt_path, pad_width)\n",
        "    X_train.append(data)\n",
        "    y_train.append(labels)\n",
        "\n",
        "X_train = np.concatenate(X_train)\n",
        "y_train = np.concatenate(y_train)\n",
        "\n",
        "# Prepare testing data\n",
        "X_test, y_test, ground_truths = [], [], []\n",
        "\n",
        "for img1_path, img2_path, gt_path in test_datasets:\n",
        "    data, labels, ground_truth = prepare_data(img1_path, img2_path, gt_path, pad_width)\n",
        "    X_test.append(data)\n",
        "    y_test.append(labels)\n",
        "    ground_truths.append(ground_truth)\n",
        "\n",
        "X_test = np.concatenate(X_test)\n",
        "y_test = np.concatenate(y_test)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "# Build and train the model\n",
        "model = unet_with_attention(input_shape=(5, 5, 2))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[lr_scheduler, early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict and evaluate\n",
        "predictions = model.predict(X_test)\n",
        "predicted_maps = (predictions > 0.5).astype(int)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "y_pred = predicted_maps.flatten()\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(18, 12))\n",
        "for idx, gt in enumerate(ground_truths):\n",
        "    plt.subplot(len(ground_truths), 2, 2 * idx + 1)\n",
        "    plt.imshow(gt, cmap='gray')\n",
        "    plt.title(f\"Ground Truth {idx + 4}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    predicted_map = predicted_maps[idx * gt.size:(idx + 1) * gt.size].reshape(gt.shape)\n",
        "    plt.subplot(len(ground_truths), 2, 2 * idx + 2)\n",
        "    plt.imshow(predicted_map, cmap='gray')\n",
        "    plt.title(f\"Predicted Change Map {idx + 4}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Function to calculate PCC, PFA, and other metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if cm.size == 4:  # Ensure it's a binary classification confusion matrix\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "        # Invert the confusion matrix for opposite labels\n",
        "        # Since predicted and ground truth are opposite\n",
        "        TN, FP, FN, TP = FP, TN, TP, FN\n",
        "\n",
        "        # Calculate metrics\n",
        "        N0 = TP + FN  # Total number of actual \"change\" pixels\n",
        "        N1 = TN + FP  # Total number of actual \"no-change\" pixels\n",
        "\n",
        "        # Probabilistic Conditional Correctness (PCC)\n",
        "        PCC = ((TP + TN) / (N0 + N1)) * 100 if (N0 + N1) != 0 else 0\n",
        "\n",
        "        # Probability of False Alarm (PFA)\n",
        "        PFA = FP / N1 if N1 != 0 else 0\n",
        "\n",
        "        # Total Error (TE)\n",
        "        PTE = (FN + FP) / (N0 + N1) if (N0 + N1) != 0 else 0\n",
        "\n",
        "        return PCC, PFA, PTE\n",
        "    else:\n",
        "        raise ValueError(\"Confusion matrix size is not 4. Ensure binary classification.\")\n",
        "\n",
        "# Initialize lists for storing results\n",
        "accuracies = []\n",
        "y_true_list = []\n",
        "y_pred_list = []\n",
        "\n",
        "# Flatten predictions and ground truths for the test set\n",
        "for idx, ground_truth in enumerate(ground_truths):\n",
        "    gt_flat = ground_truth.flatten()\n",
        "    predicted_map_flat = predicted_maps[idx * ground_truth.size:(idx + 1) * ground_truth.size].flatten()\n",
        "\n",
        "    # Ensure binary classification\n",
        "    y_true = np.where(gt_flat > 0.5, 1, 0)  # Convert to binary\n",
        "    y_pred = np.where(predicted_map_flat > 0.5, 1, 0)  # Convert to binary\n",
        "\n",
        "    # Calculate PCC, PFA, and PTE\n",
        "    PCC, PFA, PTE = calculate_metrics(y_true, y_pred)\n",
        "\n",
        "    # Calculate accuracy for the fold\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    y_true_list.append(y_true)\n",
        "    y_pred_list.append(y_pred)\n",
        "    accuracy = 1-accuracy\n",
        "\n",
        "    # Print per dataset metrics\n",
        "    print(f\"Dataset {idx + 4} Metrics:\")\n",
        "    print(f\"PCC: {PCC}%\")\n",
        "    print(f\"PFA: {PFA}\")\n",
        "    print(f\"PTE: {PTE}\")\n",
        "    print(f\"Accuracy: {accuracy * 100}%\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Print overall performance across all datasets\n",
        "all_true = np.concatenate(y_true_list)\n",
        "all_pred = np.concatenate(y_pred_list)\n",
        "\n",
        "# Calculate confusion matrix and metrics for all datasets\n",
        "cm_all = confusion_matrix(all_true, all_pred)\n",
        "TN_all, FP_all, FN_all, TP_all = cm_all.ravel()\n",
        "\n",
        "N0_all = TP_all + FN_all\n",
        "N1_all = TN_all + FP_all\n",
        "\n",
        "# Probabilistic Conditional Correctness (PCC)\n",
        "PCC_all = ((TP_all + TN_all) / (N0_all + N1_all)) * 100 if (N0_all + N1_all) != 0 else 0\n",
        "\n",
        "# Probability of False Alarm (PFA)\n",
        "PFA_all = FP_all / N1_all if N1_all != 0 else 0\n",
        "\n",
        "# Probability of Total Error (PTE)\n",
        "PTE_all = (FN_all + FP_all) / (N0_all + N1_all) if (N0_all + N1_all) != 0 else 0\n",
        "\n",
        "# Print final performance\n",
        "print(f'Final Accuracy (across all datasets): {(1-np.mean(accuracies)) * 100}%')\n",
        "print(f'Confusion Matrix (all datasets):\\n{cm_all}')\n",
        "print(f'Correct Classification (CC) across all datasets: {sum(all_true == all_pred)}')\n",
        "print(f'False Alarms (FA) across all datasets: {sum(all_pred == 1) - sum(all_true == 1)}')\n",
        "print(f'Total Error (TE) across all datasets: {sum(all_true != all_pred)}')\n",
        "print(f'Probabilistic Conditional Correctness (PCC) across all datasets: {PCC_all}%')\n",
        "print(f'Probability of False Alarm (PFA) across all datasets: {PFA_all}')\n",
        "print(f'Probability of Total Error (PTE) across all datasets: {PTE_all}')\n",
        "\n",
        "# Visualization of ground truth and predicted maps for each dataset\n",
        "plt.figure(figsize=(18, 12))\n",
        "for idx, ground_truth in enumerate(ground_truths):\n",
        "    # Plot Ground Truth\n",
        "    plt.subplot(len(ground_truths), 2, 2 * idx + 1)\n",
        "    plt.imshow(ground_truth, cmap='gray')\n",
        "    plt.title(f\"Ground Truth {idx + 4}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Plot Predicted Map\n",
        "    predicted_map = predicted_maps[idx * ground_truth.size:(idx + 1) * ground_truth.size].reshape(ground_truth.shape)\n",
        "\n",
        "    # Invert the predicted map to show the opposite\n",
        "    inverted_predicted_map = 1 - predicted_map\n",
        "\n",
        "    plt.subplot(len(ground_truths), 2, 2 * idx + 2)\n",
        "    plt.imshow(inverted_predicted_map, cmap='gray')\n",
        "    plt.title(f\"Inverted Predicted Change Map {idx + 4}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7ekPNarS3myX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}